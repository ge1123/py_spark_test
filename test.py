from pyspark.sql import SparkSession
from delta.pip_utils import configure_spark_with_delta_pip
from create_mock_delta_structure import init_mock_delta_data
import main  # å‡è¨­ main.py æœ‰ main(spark)
import os

print("ğŸ‘€ /opt/spark-warehouse å­˜åœ¨ï¼Ÿ", os.path.exists("/opt/spark-warehouse"))
print("ğŸ“‚ æ¬Šé™ï¼š", oct(os.stat("/opt/spark-warehouse").st_mode)[-3:])
print("ğŸ‘¤ æ“æœ‰è€…ï¼š", os.stat("/opt/spark-warehouse").st_uid)
builder = (
    SparkSession.builder.appName("FullETLTest")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config(
        "spark.sql.catalog.spark_catalog",
        "org.apache.spark.sql.delta.catalog.DeltaCatalog",
    )
    .config("spark.sql.catalogImplementation", "hive")
    .config("spark.sql.warehouse.dir", "/opt/spark-warehouse")  # èˆ‡ compose ä¿æŒä¸€è‡´
    .enableHiveSupport()
)

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# åˆå§‹åŒ– mock è³‡æ–™
init_mock_delta_data(spark)

# è¨»å†Š TempViewsï¼ˆä½ å¯ä»¥é¡å¤–å¯«ä¸€æ”¯ register_view.py ä¾†è™•ç†ï¼‰
spark.sql("USE test_crm_db")
spark.sql("SELECT * FROM customers").show()

# åŸ·è¡Œä¸»ç¨‹å¼é‚è¼¯
main.main()

spark.stop()
