from pathlib import Path
from jinja2 import Template
from pyspark.sql import SparkSession
import re
from typing import List, Tuple, Dict


def run_sql_template(spark: SparkSession, sql_file_path: str, params: dict):
    """
    åŸ·è¡Œæ•´ä»½ SQL æ¨¡æ¿ï¼ˆä½¿ç”¨ Jinja2 æ¸²æŸ“ï¼‰ï¼Œä¸¦ä»¥ ; æ‹†æ®µåŸ·è¡Œã€‚
    è‹¥æ ¼å¼éŒ¯èª¤æˆ–æœ‰èªæ³•éŒ¯èª¤æœƒé¡¯ç¤ºæ¸…æ¥šã€‚
    """
    sql_text = Path(sql_file_path).read_text(encoding="utf-8")
    final_sql = Template(sql_text).render(**params)

    print(f"ğŸš€ åŸ·è¡Œ SQL æª”æ¡ˆ: {sql_file_path}")
    print("------------ SQL ------------")
    print(final_sql)
    print("-----------------------------")

    # æª¢æŸ¥è‡³å°‘æœ‰ä¸€æ®µ SQL
    sql_statements = [s.strip() for s in final_sql.strip().split(";") if s.strip()]
    if not sql_statements:
        raise ValueError(
            "âŒ SQL æ¨¡æ¿åŸ·è¡Œå¤±æ•—ï¼šæœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆ SQL èªå¥ã€‚è«‹ç¢ºèªæ˜¯å¦ä»¥åˆ†è™Ÿçµå°¾ã€‚"
        )

    # åŸ·è¡Œæ¯æ®µ SQLï¼Œè‹¥å¤±æ•—å°±å°å‡ºæ˜¯å“ªä¸€æ®µéŒ¯
    for i, stmt in enumerate(sql_statements, start=1):
        try:
            print(f"\nâ–¶ åŸ·è¡Œç¬¬ {i} æ®µ SQLï¼š")
            print(stmt)
            spark.sql(stmt)
        except Exception as e:
            print(f"âŒ ç¬¬ {i} æ®µ SQL åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")
            raise


def parse_etl_steps(sql_file_path: str) -> List[Tuple[str, str]]:
    """
    æ‹†è§£ SQL æª”æ¡ˆï¼Œå–å¾—æ‰€æœ‰ STEP åç¨±èˆ‡å°æ‡‰ SQL å…§å®¹ã€‚
    è‹¥åµæ¸¬æœ‰æœªåŒ…è£é€² STEP çš„ SQL å€æ®µæˆ– STEP åç¨±å«ç©ºæ ¼ï¼Œæœƒæ‹‹å‡ºéŒ¯èª¤ã€‚
    """
    sql_text = Path(sql_file_path).read_text(encoding="utf-8")

    # æ‰¾å‡ºæ‰€æœ‰ STEP å€å¡Š
    step_blocks = re.findall(
        r"--\s*\[STEP:\s*(.*?)\s*\](.*?)(?=--\s*\[STEP:|\Z)",
        sql_text,
        re.DOTALL,
    )

    # ğŸ”’ æª¢æŸ¥æ¯å€‹ step name æ˜¯å¦å«æœ‰ç©ºæ ¼
    for step_name, _ in step_blocks:
        if " " in step_name.strip():
            raise ValueError(
                f"âŒ STEP åç¨±ä¸åˆæ³•ï¼š`{step_name}` å«æœ‰ç©ºç™½ï¼Œè«‹æ”¹ç‚ºä¾‹å¦‚ INSERT_INTO æˆ– VW_CUSTOMERS"
            )

    # æª¢æŸ¥æ‰€æœ‰ SQL å€æ®µç¸½é‡ â‰ˆ STEP å€å¡Šæ•¸
    raw_sql_without_steps = re.sub(r"--\s*\[STEP:\s*.*?\s*\]", "", sql_text)
    all_sql_stmts = [
        s.strip() for s in raw_sql_without_steps.strip().split(";") if s.strip()
    ]

    if len(all_sql_stmts) > len(step_blocks):
        raise ValueError(
            f"âŒ æª¢æŸ¥å¤±æ•—ï¼šåµæ¸¬åˆ° {len(all_sql_stmts)} æ®µ SQLï¼Œä½†åªæœ‰ {len(step_blocks)} å€‹ STEPã€‚\n"
            f"è«‹ç¢ºèªæ‰€æœ‰ SQL éƒ½æœ‰å°æ‡‰çš„ `-- [STEP: NAME]` å€å¡Šæ¨™ç±¤ï¼Œä¸”æ ¼å¼æ­£ç¢ºã€‚"
        )

    return step_blocks


def execute_step(
    spark: SparkSession, step_name: str, step_sql: str, params: Dict
) -> bool:
    """
    åŸ·è¡Œå–®ä¸€ STEPã€‚å‚³å› True è¡¨ç¤ºæˆåŠŸï¼ŒFalse è¡¨ç¤ºéŒ¯èª¤ã€‚
    """
    print(f"\nğŸš€ STEP: {step_name.strip()}")
    try:
        final_sql = Template(step_sql.strip()).render(**params)
        print(final_sql)
        spark.sql(final_sql)
        return True
    except Exception as e:
        print(f"âŒ STEP {step_name.strip()} åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")
        return False


def get_step_by_name(sql_file_path: str, target_step: str) -> Tuple[str, str]:
    """
    æ ¹æ“š STEP åç¨±å¾ SQL æª”æ¡ˆä¸­æ“·å–å°æ‡‰çš„ step_name å’Œ step_sqlã€‚
    è‹¥æ‰¾ä¸åˆ°å‰‡ raise éŒ¯èª¤ã€‚
    """
    steps = parse_etl_steps(sql_file_path)
    for step_name, step_sql in steps:
        if step_name.strip().upper() == target_step.strip().upper():
            return step_name, step_sql
    raise ValueError(f"âŒ æ‰¾ä¸åˆ° STEP: {target_step}")


def run_single_step(
    spark: SparkSession, sql_file_path: str, step_name: str, params: Dict
):
    """
    å¾ SQL æª”æ¡ˆä¸­æŠ“å‡ºæŒ‡å®š STEPï¼Œä¸¦åŸ·è¡Œå®ƒã€‚
    """
    matched_step_name, step_sql = get_step_by_name(sql_file_path, step_name)
    return execute_step(spark, matched_step_name, step_sql, params)
