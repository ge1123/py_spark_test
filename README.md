# âœ… å•Ÿå‹• Spark å®¹å™¨ï¼ˆä½¿ç”¨ docker-composeï¼‰
```bash
docker compose -f docker-compose.spark.yml up --build -d
```

# âœ… é€²å…¥ Spark å®¹å™¨
```bash
docker exec -it spark bash
```

# âœ… åˆ‡æ›åˆ°å°ˆæ¡ˆç›®éŒ„ï¼ŒåŸ·è¡Œ ETL è…³æœ¬ï¼ˆä½¿ç”¨ Pythonï¼‰
```bash
cd /opt/project
python test.py
```

# âœ… é—œé–‰ Spark å®¹å™¨
```bash
docker compose -f docker-compose.spark.yml down
```

# âœ… é‡æ–°å»ºæ§‹ Spark å®¹å™¨ï¼ˆæ¸…é™¤å¿«å–ï¼‰
```bash
docker compose -f docker-compose.spark.yml build --no-cache
```

# âœ… å®‰è£æŒ‡å®šç‰ˆæœ¬ï¼ˆåœ¨å®¹å™¨å…§åŸ·è¡Œï¼‰
```bash
pip install pyspark==3.4.2 delta-spark==2.4.0
```

# âœ… æª¢æŸ¥ç’°å¢ƒç‰ˆæœ¬ï¼ˆåœ¨å®¹å™¨å…§ï¼‰
python --version
pyspark --version
pip show delta-spark
spark-shell --version

# âœ… æ¸¬è©¦ delta-core æ˜¯å¦è¢« ivy ä¸‹è¼‰ï¼ˆå¯é¸ï¼‰
find ~/.ivy2.5.2 -name "delta-*.jar"
<!-- 
# ğŸš« ç”¨ä¸åˆ°çš„ï¼ˆå·²æ£„ç”¨ï¼Œåƒ…å‚™æŸ¥ï¼‰
# spark-submit \
#   --packages io.delta:delta-core_2.12:2.4.0 \
#   --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
#   --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
#   --conf spark.hadoop.delta.logStore.class=org.apache.spark.sql.delta.storage.LocalLogStore \
#   test.py

# spark-submit $SPARK_SUBMIT_OPTIONS test.py -->
